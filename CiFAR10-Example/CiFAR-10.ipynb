{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpickle(file):\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = pickle.load(fo, encoding='bytes')\n",
    "    return dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"cifar-10-batches-py/data_batch_1\"\n",
    "data = unpickle(path)\n",
    "\n",
    "\n",
    "testPath = \"cifar-10-batches-py/test_batch\"\n",
    "testData = unpickle(testPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = data[list(data.keys())[1]]\n",
    "images = data[list(data.keys())[2]]\n",
    "\n",
    "testLabels = testData[list(testData.keys())[1]]\n",
    "testImages = testData[list(testData.keys())[2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[154, 126, 105],\n",
       "        [102, 125, 155],\n",
       "        [172, 180, 142],\n",
       "        ...,\n",
       "        [ 88, 103,  94],\n",
       "        [ 65,  83,  90],\n",
       "        [ 79,  68,  67]],\n",
       "\n",
       "       [[136, 137, 122],\n",
       "        [132, 151, 181],\n",
       "        [203, 208, 208],\n",
       "        ...,\n",
       "        [ 92,  88,  78],\n",
       "        [ 87,  98,  76],\n",
       "        [ 67,  81,  91]],\n",
       "\n",
       "       [[146, 124,  88],\n",
       "        [ 85,  87,  84],\n",
       "        [ 75,  78,  69],\n",
       "        ...,\n",
       "        [169, 113,  89],\n",
       "        [ 84,  65,  56],\n",
       "        [ 88,  81,  63]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[158,  83,  55],\n",
       "        [ 46,  51,  52],\n",
       "        [ 48,  46,  57],\n",
       "        ...,\n",
       "        [134, 121,  61],\n",
       "        [ 51,  33,  21],\n",
       "        [ 53,  51, 107]],\n",
       "\n",
       "       [[172, 166, 123],\n",
       "        [140, 160, 155],\n",
       "        [139, 131, 126],\n",
       "        ...,\n",
       "        [ 96, 101, 135],\n",
       "        [ 87,  78,  29],\n",
       "        [ 84,  73,  94]],\n",
       "\n",
       "       [[166, 160, 170],\n",
       "        [163, 165, 171],\n",
       "        [180, 186, 174],\n",
       "        ...,\n",
       "        [ 42,  67, 101],\n",
       "        [122, 133, 136],\n",
       "        [139, 142, 144]]], dtype=uint8)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pic = 1\n",
    "\n",
    "print(labels[pic])\n",
    "images[pic].reshape(32,32,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "scaler.fit(images)\n",
    "\n",
    "trainX = scaler.transform(images)\n",
    "testX = scaler.transform(testImages)\n",
    "\n",
    "encoder = OneHotEncoder()\n",
    "encoder.fit(np.array(labels).reshape(-1, 1))\n",
    "\n",
    "trainY = encoder.transform(np.array(labels).reshape(-1, 1)).toarray()\n",
    "testY = encoder.transform(np.array(testLabels).reshape(-1, 1)).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reshape data for CNN\n",
    "trainX = trainX[..., None]\n",
    "\n",
    "testX = testX[..., None]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dropout\n",
    "from tensorflow.keras import layers, models\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_epochs = 10\n",
    "lr = .01\n",
    "    \n",
    "model = Sequential()\n",
    "\n",
    "model.add((Conv1D(filters=64, kernel_size=5, activation='relu')))\n",
    "model.add((Conv1D(filters=32, kernel_size=4, activation='relu')))\n",
    "model.add((Conv1D(filters=16, kernel_size=3, activation='relu')))\n",
    "model.add((MaxPooling1D(pool_size=2)))\n",
    "model.add((Conv1D(filters=8, kernel_size=2, activation='relu')))\n",
    "model.add((MaxPooling1D(pool_size=2)))\n",
    "model.add((Conv1D(filters=4, kernel_size=3, activation='relu')))\n",
    "model.add((MaxPooling1D(pool_size=2)))\n",
    "\n",
    "#output\n",
    "model.add(Flatten())\n",
    "model.add(Dense(10, activation='sigmoid'))\n",
    "\n",
    "adam = tf.optimizers.Adam(learning_rate=lr, beta_1=0.9, beta_2=0.999, epsilon=10e-7, decay=0.01)\n",
    "\n",
    "model.compile(optimizer=adam, loss='categorical_crossentropy', metrics = [\"Precision\",\"Recall\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      " 8/79 [==>...........................] - ETA: 1:04 - loss: 2.3015 - precision: 0.1112 - recall: 0.4766"
     ]
    }
   ],
   "source": [
    "model.fit(trainX, trainY, epochs = num_epochs, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 11s 33ms/step - loss: 1.5775 - precision: 0.3718 - recall: 0.5402\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.5775079727172852, 0.37180811166763306, 0.5401999950408936]"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(testX,testY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(testX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4617\n"
     ]
    }
   ],
   "source": [
    "error = 0\n",
    "\n",
    "for i in range(0,len(pred)):\n",
    "    if np.argmax(pred[i]) != testLabels[i]:\n",
    "        error += 1\n",
    "\n",
    "\n",
    "acc = 1 - (error/len(pred))\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
